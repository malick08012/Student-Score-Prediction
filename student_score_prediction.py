# -*- coding: utf-8 -*-
"""Student Score Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_WMw0WXV0JB5H6K_7PHzKNhBN6JbLHM

**Upload Dataset**
"""

from google.colab import files
uploaded = files.upload()

"""**Import Required Libraries**




"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

"""**Load The Dataset**"""

df=pd.read_csv("/content/StudentPerformanceFactors.csv")
df.head()

"""**Basic Data Understanding**"""

df.shape
df.info()
df.isnull().sum()

"""**Required Columns Only**"""

df = df[["Hours_Studied", "Exam_Score"]]
df.head()

"""**Visualization**"""

plt.figure(figsize=(8,5))
plt.scatter(df['Hours_Studied'], df['Exam_Score'])
plt.xlabel("Hours Studied")
plt.ylabel("Exam Score")
plt.title("Hours Studied vs Exam Score")
plt.show()

"""**Train/Test Split**"""

X = df[['Hours_Studied']]
y = df['Exam_Score']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""**Train Linear Regression Model**"""

model=LinearRegression()
model.fit(X_train, y_train)

"""**Make Predictions**"""

y_pred = model.predict(X_test)

"""**Defining Mean Absolute Error**"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""**Evaluate Model**"""

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("RMSE:", rmse)
print("R2 Score:", r2)

"""**Coclusion:**

The linear regression model shows a positive relationship between hours studied and exam score. However, the R² score of 0.23 indicates that study hours alone explain only 23% of the variation in exam performance. This suggests that other academic and personal factors also significantly influence exam outcomes.

**Bonus Task**

**Polynomial Regression**

Import Polynomial Features
"""

from sklearn.preprocessing import PolynomialFeatures

"""**Create Polynomial Features (Degree 2)**

We will try degree = 2 (quadratic curve).

Explanation:

Now model becomes:

Exam_Score = a(Hours) + b(Hours²) + c
"""

poly = PolynomialFeatures(degree=2)

X_poly = poly.fit_transform(X)

X_train_poly, X_test_poly, y_train, y_test = train_test_split(
    X_poly, y, test_size=0.2, random_state=42
)

"""**Train Polynomial Regression Model**"""

poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)

"""**Make Predictions**"""

y_pred_poly = poly_model.predict(X_test_poly)

"""**Evaluate Polynomial Model**"""

mae_poly = mean_absolute_error(y_test, y_pred_poly)
mse_poly = mean_squared_error(y_test, y_pred_poly)
rmse_poly = mse_poly ** 0.5
r2_poly = r2_score(y_test, y_pred_poly)

print("Polynomial Regression Results")
print("MAE:", mae_poly)
print("RMSE:", rmse_poly)
print("R2 Score:", r2_poly)

"""**Results:**

Linear R² → 0.2319
Polynomial R² → 0.2327

Difference = almost nothing

Polynomial regression did NOT significantly improve performance.

That tells us:

The relationship between Hours_Studied and Exam_Score is mostly linear.

There is no strong curve pattern.

The main limitation is not model type — it's missing important features.

**Visualizing Polynomial Regression Curve**
"""

import numpy as np

# Create smooth range of study hours
X_range = np.linspace(df['Hours_Studied'].min(),
                      df['Hours_Studied'].max(),
                      100).reshape(-1, 1)

# Transform to polynomial features
X_range_poly = poly.transform(X_range)

# Predict using polynomial model
y_range_pred = poly_model.predict(X_range_poly)

# Plot
plt.figure(figsize=(8,5))

# Original data points
plt.scatter(df['Hours_Studied'], df['Exam_Score'],
            color='blue', alpha=0.4, label='Actual Data')

# Polynomial curve
plt.plot(X_range, y_range_pred,
         color='red', linewidth=3, label='Polynomial Curve')

plt.xlabel("Hours Studied")
plt.ylabel("Exam Score")
plt.title("Polynomial Regression Curve")
plt.legend()
plt.show()

"""**Experiment With Diffrent Feature**

Reload Dataset
"""

df = pd.read_csv("StudentPerformanceFactors.csv")
df.head()

"""**Choosing a Strong Feature ( Previous_Scores )**"""

# Selecting two features now
X_multi = df[['Hours_Studied', 'Previous_Scores']]
y = df['Exam_Score']

X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(
    X_multi, y, test_size=0.2, random_state=42
)

"""**Train Linear Regression Again**"""

multi_model = LinearRegression()
multi_model.fit(X_train_m, y_train_m)

"""**Make Predictions**"""

y_pred_multi = multi_model.predict(X_test_m)

"""**Evaluate New Model**"""

mae_multi = mean_absolute_error(y_test_m, y_pred_multi)
mse_multi = mean_squared_error(y_test_m, y_pred_multi)
rmse_multi = mse_multi ** 0.5
r2_multi = r2_score(y_test_m, y_pred_multi)

print("Multiple Linear Regression Results")
print("MAE:", mae_multi)
print("RMSE:", rmse_multi)
print("R2 Score:", r2_multi)

"""**Conclusion**

Adding Previous_Scores as an additional feature slightly improved model performance, increasing the R² score from 0.23 to 0.25. However, the improvement was modest, indicating that exam performance is influenced by multiple interacting factors rather than just study hours or previous scores alone. This highlights the importance of feature selection in regression modeling.
"""